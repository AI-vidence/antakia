{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AntakIA ! AntakIA is a software allowing you to explore a trained machine learning (ML) model and discover what it has understood from the training data. A key use case is to explain (one speaks of 'explainability' of AI) the model to have it approved and certified to be compliant with the law. As a matter of fact, Europe should issue by the beginning of 2024 or its 'AI act'. This text will require companies, when they use AI for certain use cases, to have their model certified. AntakIA is a solution for this compliance issue. But it does more. For instancee it allows datascientist to better understand their dataset and model. Marketing teams can get new insights on their customers etc. AntakIA is an open-source project by AI-vidence, a French company, showcasing our regional approach for AI explainability. Besides local and global explainability, we believe in a more relevant and intermediate scale to observe phenomena. We divide the multidimensional space of input values in regions where we can substitute the original 'black box' model with simmpler and explainable models. Because we split the space in smaller pieces, we named our approach from the antique city Antakia (in T\u00fcrkiye nowadays) famous for its roman mosaics. Now, you're only a pip install antakia away from AntakIA :) ... See our getting started page for detailed instructions Contents Below is what you'll find in this documentation : * Getting started : install and usage * A step by step tutorial : first part and second part * Advanced usage * Regional explainability and the science behind it * About AI-vidence, the team behind AntakIA * Credits","title":"Home"},{"location":"#welcome-to-antakia","text":"AntakIA is a software allowing you to explore a trained machine learning (ML) model and discover what it has understood from the training data. A key use case is to explain (one speaks of 'explainability' of AI) the model to have it approved and certified to be compliant with the law. As a matter of fact, Europe should issue by the beginning of 2024 or its 'AI act'. This text will require companies, when they use AI for certain use cases, to have their model certified. AntakIA is a solution for this compliance issue. But it does more. For instancee it allows datascientist to better understand their dataset and model. Marketing teams can get new insights on their customers etc. AntakIA is an open-source project by AI-vidence, a French company, showcasing our regional approach for AI explainability. Besides local and global explainability, we believe in a more relevant and intermediate scale to observe phenomena. We divide the multidimensional space of input values in regions where we can substitute the original 'black box' model with simmpler and explainable models. Because we split the space in smaller pieces, we named our approach from the antique city Antakia (in T\u00fcrkiye nowadays) famous for its roman mosaics. Now, you're only a pip install antakia away from AntakIA :) ... See our getting started page for detailed instructions","title":"Welcome to AntakIA !"},{"location":"#contents","text":"Below is what you'll find in this documentation : * Getting started : install and usage * A step by step tutorial : first part and second part * Advanced usage * Regional explainability and the science behind it * About AI-vidence, the team behind AntakIA * Credits","title":"Contents"},{"location":"about/","text":"About AntakIA","title":"About"},{"location":"about/#about-antakia","text":"","title":"About AntakIA"},{"location":"advanced/","text":"","title":"Advanced"},{"location":"credits/","text":"","title":"Credits"},{"location":"getting_started/","text":"Installation Install fron PyPi (recommended) pip install antakia Install from source Clone our repo : git clone https://github.com/AI-vidence/antakia.git cd antakia If you use poetry : poetry shell poetry install If you prefer pip : python3.10 -m venv . # or >3.10 source bin/activate pip install -e . Starting with example notebooks AntakIA is designed to work within notebooks (Jupyter or Google Collab for example) Then you'll need a notebook to work with AntaakIA. If you cloned our repo you'll find in the ~/examples/ folder some sample notebooks ( .ipynb files). Otherwise, you can download them manually from our Github page . If you're note used with notebooks, here are common ways to run them * locally : * right from Visual Studio Code * on a local Jupyter server * on an online service hosting a notebook server : Google collab, a Jupyter hub (for many user) install in your university or company Running notebooks in Visual Studio Code Open the notebook file (ending with the ipynb extension) in the file explorer on the left: At the top right corner of the VScode windows, choose the Python kernel you wan't to use. Select your normal Python interpretor if you installed antakia through pip install . If you downloaded the code and have installed it by yoursel, make sure you chose the virtual environment you set up. Then you can run the nootebok with these buttons : Running notebooks in a local Jupyter server Go in your working directory where you have your data and notebooks. cd <your_directory> pip install notebook Then launch the server with : jupyter notebook","title":"Installation"},{"location":"getting_started/#installation","text":"","title":"Installation"},{"location":"getting_started/#install-fron-pypi-recommended","text":"pip install antakia","title":"Install fron PyPi (recommended)"},{"location":"getting_started/#install-from-source","text":"Clone our repo : git clone https://github.com/AI-vidence/antakia.git cd antakia If you use poetry : poetry shell poetry install If you prefer pip : python3.10 -m venv . # or >3.10 source bin/activate pip install -e .","title":"Install from source"},{"location":"getting_started/#starting-with-example-notebooks","text":"AntakIA is designed to work within notebooks (Jupyter or Google Collab for example) Then you'll need a notebook to work with AntaakIA. If you cloned our repo you'll find in the ~/examples/ folder some sample notebooks ( .ipynb files). Otherwise, you can download them manually from our Github page . If you're note used with notebooks, here are common ways to run them * locally : * right from Visual Studio Code * on a local Jupyter server * on an online service hosting a notebook server : Google collab, a Jupyter hub (for many user) install in your university or company","title":"Starting with example notebooks"},{"location":"getting_started/#running-notebooks-in-visual-studio-code","text":"Open the notebook file (ending with the ipynb extension) in the file explorer on the left: At the top right corner of the VScode windows, choose the Python kernel you wan't to use. Select your normal Python interpretor if you installed antakia through pip install . If you downloaded the code and have installed it by yoursel, make sure you chose the virtual environment you set up. Then you can run the nootebok with these buttons :","title":"Running notebooks in Visual Studio Code"},{"location":"getting_started/#running-notebooks-in-a-local-jupyter-server","text":"Go in your working directory where you have your data and notebooks. cd <your_directory> pip install notebook Then launch the server with : jupyter notebook","title":"Running notebooks in a local Jupyter server"},{"location":"regional_explain/","text":"","title":"Regional explain"},{"location":"tutorial1/","text":"Tutorial This is the part 1 of our tutorial. It explains how to prepare data and launch AntakIA. Those steps are common to most of AntakIA uses. If you feel familiar enough, you can directly jump to second part. The California housing dataset We'll use the California housing dataset, very famous in the datascience ecosystem. This dataset describes 20 640 block groups (ie. groups of houses) in California using 8 variables : * MedInc median income in block group * HouseAge median house age in block group * AveRooms average number of rooms per household * AveBedrms average number of bedrooms per household * Population block group population * AveOccup average number of household members * Latitude block group latitude * Longitude block group longitude The dataset also gives for each block group the average price of a house. This data comes from real real estate transactions. In our noteboox, this dataset is stored in a Pandas Dataframe named X . If you type X.head() you'll get : The \"medium house values\" are stored in a Pandas Series named y . A y.head() will give you somethin like : The use case We can imagine several use cases where AntakIA could be very useful. For instance : * Let's say you're a real estate agent in California. A datascientist in your team has trained a wonderful ML model that is capable to predict the market value of any house in the state, as long as you provide sufficent data. You're amazed and want to unnderstand how this model works in order to gain insights of your market : what drives the price ? any segmentation ? So you decided to use AntakIA. * Or, you don't have such model. But you still want to have an accurate understanding of your market. Then you ask a datascientist to train a model. ANd then you use AntakIA on it. It's quite the same story : you have dataset X , you do a supervised training ( X , y ) to get a fitted model M. AntakIA will help you to understand how and why M can predict house values. Preparing the data Open the file california_housing.ipynb (in the examples folder of the code, or wherever you downloaded it) Let's analyze the first cells : import pandas as pd df = pd.read_csv('../data/california_housing.csv').drop(['Unnamed: 0'], axis=1) ```` We start creating a dataframe from a local CSV file. You could have imported this dataset from the Scikit-learn package [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html). As you'll see, AntakIA needs to compute other values calues (eg. SHAP values for the data and the model). So make this tutorial quicker and more pleaseant, our CSV file includes these values pre-computed. Remove outliers: df = df.loc[df['Population']<10000] df = df.loc[df['AveOccup']<6] df = df.loc[df['AveBedrms']<1.5] df = df.loc[df['HouseAge']<50] # Only San Francisco : df = df.loc[(df['Latitude']<38.07)&(df['Latitude']>37.2)] df = df.loc[(df['Longitude']>-122.5)&(df['Longitude']<-121.75)] In the same way, the previous lines are not compulsory. But it appears that the dataset for the sole city of San Francisco is better to get rapidly a good intuition of how AntakIA works. X = df.iloc[:,0:8] # the dataset y = df.iloc[:,9] # the target variable shap_values = df.iloc[:,[10,11,12,13,14,15,16,17]] # the SHAP values `` Here we have extracted from our big CSV dataset : the X values, the y Series and the shap_values` (we'll explain those values further).","title":"Tutorial"},{"location":"tutorial1/#tutorial","text":"This is the part 1 of our tutorial. It explains how to prepare data and launch AntakIA. Those steps are common to most of AntakIA uses. If you feel familiar enough, you can directly jump to second part.","title":"Tutorial"},{"location":"tutorial1/#the-california-housing-dataset","text":"We'll use the California housing dataset, very famous in the datascience ecosystem. This dataset describes 20 640 block groups (ie. groups of houses) in California using 8 variables : * MedInc median income in block group * HouseAge median house age in block group * AveRooms average number of rooms per household * AveBedrms average number of bedrooms per household * Population block group population * AveOccup average number of household members * Latitude block group latitude * Longitude block group longitude The dataset also gives for each block group the average price of a house. This data comes from real real estate transactions. In our noteboox, this dataset is stored in a Pandas Dataframe named X . If you type X.head() you'll get : The \"medium house values\" are stored in a Pandas Series named y . A y.head() will give you somethin like :","title":"The California housing dataset"},{"location":"tutorial1/#the-use-case","text":"We can imagine several use cases where AntakIA could be very useful. For instance : * Let's say you're a real estate agent in California. A datascientist in your team has trained a wonderful ML model that is capable to predict the market value of any house in the state, as long as you provide sufficent data. You're amazed and want to unnderstand how this model works in order to gain insights of your market : what drives the price ? any segmentation ? So you decided to use AntakIA. * Or, you don't have such model. But you still want to have an accurate understanding of your market. Then you ask a datascientist to train a model. ANd then you use AntakIA on it. It's quite the same story : you have dataset X , you do a supervised training ( X , y ) to get a fitted model M. AntakIA will help you to understand how and why M can predict house values.","title":"The use case"},{"location":"tutorial1/#preparing-the-data","text":"Open the file california_housing.ipynb (in the examples folder of the code, or wherever you downloaded it) Let's analyze the first cells : import pandas as pd df = pd.read_csv('../data/california_housing.csv').drop(['Unnamed: 0'], axis=1) ```` We start creating a dataframe from a local CSV file. You could have imported this dataset from the Scikit-learn package [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html). As you'll see, AntakIA needs to compute other values calues (eg. SHAP values for the data and the model). So make this tutorial quicker and more pleaseant, our CSV file includes these values pre-computed.","title":"Preparing the data"},{"location":"tutorial1/#remove-outliers","text":"df = df.loc[df['Population']<10000] df = df.loc[df['AveOccup']<6] df = df.loc[df['AveBedrms']<1.5] df = df.loc[df['HouseAge']<50]","title":"Remove outliers:"},{"location":"tutorial1/#only-san-francisco","text":"df = df.loc[(df['Latitude']<38.07)&(df['Latitude']>37.2)] df = df.loc[(df['Longitude']>-122.5)&(df['Longitude']<-121.75)] In the same way, the previous lines are not compulsory. But it appears that the dataset for the sole city of San Francisco is better to get rapidly a good intuition of how AntakIA works. X = df.iloc[:,0:8] # the dataset y = df.iloc[:,9] # the target variable shap_values = df.iloc[:,[10,11,12,13,14,15,16,17]] # the SHAP values `` Here we have extracted from our big CSV dataset : the X values, the y Series and the shap_values` (we'll explain those values further).","title":"# Only San Francisco :"},{"location":"tutorial1/#_1","text":"","title":""},{"location":"tutorial2/","text":"","title":"Tutorial2"}]}