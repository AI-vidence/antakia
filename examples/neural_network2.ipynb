{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=NumbaDeprecationWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/mnist/mnist_train.csv')\n",
    "data_test = pd.read_csv('../data/mnist/mnist_test.csv')\n",
    "\n",
    "N_train = 6000\n",
    "n_train = N_train/len(data_train)\n",
    "N_test = 2000\n",
    "n_test = N_test/len(data_test)\n",
    "\n",
    "data_train = data_train.sample(frac=n_train, random_state=9)\n",
    "data_test = data_test.sample(frac=n_test, random_state=9)\n",
    "\n",
    "X_train = data_train.iloc[:, 1:].values\n",
    "y_train = data_train.iloc[:, 0].values\n",
    "\n",
    "X_test = data_test.iloc[:, 1:].values\n",
    "y_test = data_test.iloc[:, 0].values\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(X_train)\n",
    "\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "def transform_line(line, seuil, taille_contour):\n",
    "    # [moyenne, ecart-type, nb de blanc, nb de noir, contour, centre]\n",
    "    l_tot = []\n",
    "    l_tot.append(line.mean())\n",
    "    l_tot.append(line.std())\n",
    "    l_tot.append(np.count_nonzero(line))\n",
    "    a = 0\n",
    "    for j in range(len(line)):\n",
    "        if line[j] > seuil :\n",
    "            a+=1\n",
    "    l_tot.append(a)\n",
    "    m_cont=[]\n",
    "    m_centre=[]\n",
    "    for j in range(len(line)):\n",
    "        if j < taille_contour*28 or j > 783 - taille_contour*28 or j%28 < taille_contour or j%28 > 27 - taille_contour:\n",
    "            m_cont.append(line[j])\n",
    "        else :\n",
    "            m_centre.append(line[j])\n",
    "    l_tot.append(np.mean(m_cont))\n",
    "    l_tot.append(np.mean(m_centre))\n",
    "    return np.round(l_tot,4)\n",
    "\n",
    "def transfo_perso(df, seuil, taille_contour):\n",
    "    new_X = pd.DataFrame(columns=['moyenne', 'ecart-type', 'nb de blanc', 'nb de noir', 'contour', 'centre'])\n",
    "    for i in range(len(df)):\n",
    "        new_X.loc[i] = transform_line(df[i], seuil, taille_contour)\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = transfo_perso(X_train, 200, 4)\n",
    "new_X_test = transfo_perso(X_test, 200, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(new_X_train)\n",
    "X_train = scaler.transform(new_X_train)\n",
    "X_test = scaler.transform(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 10\n",
    "NUM_FEATURES = 6\n",
    "RANDOM_SEED = 9\n",
    "\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "X_train = torch.from_numpy(X_train).type(torch.float)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.float)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "print(X_train[:5], y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units), # how many classes are there?\n",
    "            nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features),\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model = Model(input_features=NUM_FEATURES, \n",
    "                    output_features=NUM_CLASSES, \n",
    "                    hidden_units=20)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    ### Training\n",
    "    model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model(X_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "        # 2. Calculate test loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,y_pred=test_pred)\n",
    "\n",
    "    if epoch % (epochs/10) == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_test)\n",
    "y = pd.Series(y_test)\n",
    "X.columns = ['moyenne', 'ecart-type', 'nb de blanc', 'nb de noir', 'contour', 'centre']\n",
    "\n",
    "class MonModel():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X.values).float()\n",
    "        return pd.DataFrame((self.model(X).detach().numpy())).idxmax(axis=1)\n",
    "    \n",
    "my_model = MonModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antakia\n",
    "explain = antakia.Xplainer(X, y, my_model)\n",
    "explain.interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = explain.result()[3]['y']\n",
    "l = list(l)\n",
    "for i in range(10):\n",
    "    print(f'{i} : {l.count(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = explain.result()[3]['X']\n",
    "y_1 = explain.result()[3]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
